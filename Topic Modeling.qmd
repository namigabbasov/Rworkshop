---
title: "Text Analysis 3: Topic Modeling"
format: html
editor: visual
---

```{r}
rm(list = ls())
setwd("/Users/namigabbasov/Desktop/Workshops/Text Analysis 3")
library(caret) 
library(glmnet)
library(pROC)
library(plyr)
library(dplyr)
library(quanteda)
library(readtext)
library(FactoMineR)
library(factoextra)
library(stm)
```

## Load Data

```{r}
data_dir<-"/Users/namigabbasov/Desktop/Workshops/Text Analysis 2/"                             ### set working directory for data 
UNGD<- readtext(paste0(data_dir, "UNGD/*"),                                                    ### import txt files
                             docvarsfrom = "filenames", 
                             dvsep="_", 
                             docvarnames = c("ccodealp", "session", "year"))
```

## Create Corpus

```{r}
UNGD_corpus <- corpus(UNGD, text_field = "text")                          ### create quantida corpus 
summary(UNGD_corpus)
```

## Preprocessing Corpus and Creating Document Feature Matrix(DFM)

```{r}
ungd_tokens <- tokens(UNGD_corpus,                                     ### tokenize 
                       what = "word",    
                       remove_punct = TRUE,                            ### remove punctuation  
                       remove_symbols = TRUE,                          ### remove symbols 
                       remove_numbers = TRUE,                          ### remove numbers 
                       remove_url = TRUE,                              ### remove urls 
                       remove_separators = TRUE)    


ungd_dfm<- dfm(ungd_tokens,                                           ### lowercase the terms and create DFM
                       tolower = TRUE)

ungd_dfm<<-  dfm_remove(ungd_dfm,                                     ### remove stopwords and custom words and symbols 
                        pattern = c(stopwords("english"), "--", "must", "like", "will", "also", "a1", "abba", "abdel", "'" , "-")) 

ungd_dfm<- dfm_wordstem(ungd_dfm,                                     ### stemming  
                        language = "en")    

ungd_dfm<- dfm_trim(ungd_dfm,
                      min_termfreq = 5)                              ### remove infrequent words
                      #max_docfreq = "")                             ### remove frequent words 
```

## Baisc Topic Modeling with LDA

```{r}
ungd2022 <- dfm_subset(ungd_dfm, year ==2022)                                                         ### sub-setting year 2022
#ungd_dem<- dfm_subset(ungd2001,regime_status_name=="Democracy")                                      ### sub-setting democratic countries 
#ungd_auth<- dfm_subset(ungd2001,regime_status_name=="Authoritarian Regime")                          ### sub-setting authoritarian regimes 


lda_model<- stm(ungd2022,                                                                             ### fit a topic model with k = 5 topics
               K = 5,
               init.type = "LDA", 
               seed = 12345,
               verbose = TRUE)
```

```{r}
summary(lda_model)
### Highest prob: Highest probability terms 
### FREX: Frequent and exclusive terms
### Lift:  Less frequent terms in other topics 
### Score: Log frequency of word in the topic divided by log frequency of the word in other topics
```

## Interpretation

1.  **Topic 1 - Climate Impact on Island Nations:**

    -   **Core Concepts:** Development, climate change, global challenges, and their impact on nations.

    -   **Regional Focus:** Primarily islands and regions in the Pacific Ocean.

    -   **Specific Insights:** This topic appears to focus on the vulnerabilities and challenges faced by small island nations due to climate change. Terms such as 'sid', 'ocean', and 'solomon' suggest a special emphasis on Small Island Developing States and the Pacific islands.

2.  **Topic 2 - Geopolitical Conflicts and Regional Dynamics:**

    -   **Core Concepts:** International relations, peace, and geopolitical conflicts.

    -   **Regional Focus:** Countries such as Azerbaijan, Armenia, Iraq, Syria, Israel, Iran.

    -   **Specific Insights:** This topic delves into the intricate geopolitical tensions, particularly in the Middle East and surrounding regions. The presence of 'azerbaijan' and 'armenia' hints at discussions related to the Nagorno-Karabakh conflict.

3.  **Topic 3 - European Political Landscape and Conflicts:**

    -   **Core Concepts:** Wars, global security, and European geopolitics.

    -   **Regional Focus:** Primarily European countries, with an emphasis on Ukraine and Russia.

    -   **Specific Insights:** This topic provides insights into the political tension between Russia and Ukraine, as evident from 'ukrainian' and 'russia'. It also touches upon the broader European political landscape, including the EU.

4.  **Topic 4 - African Security and Development:**

    -   **Core Concepts:** International peace, security, and developmental challenges.

    -   **Regional Focus:** Primarily African nations like Mali, Congo, Sudan, and Central African Republic.

    -   **Specific Insights:** The topic centers on the political and social challenges in various African nations. Issues related to peacekeeping, developmental challenges, and security concerns in the African continent seem to be the focus.

5.  **Topic 5 - Global Development and Agriculture:**

    -   **Core Concepts:** Global development, education, and agriculture.

    -   **Regional Focus:** Broader global context, with mentions of specific countries like Bhutan, Guyana, and Namibia.

    -   **Specific Insights:** This topic looks at the challenges and opportunities in education, agricultural productivity, and broader developmental goals. The emphasis on 'food', 'agriculture', and 'product' indicates discussions around food security and sustainable agriculture.

## Topic Quality

```{r}
topicQuality(model=lda_model, documents=ungd2022)
```

## Topic proportions

```{r}
plot.STM(lda_model,
         type="summary",
         labeltype = "frex",
         n = 5)
```

## Structural Topic Models(STM): Preparing Data

```{r}
install.packages("readxl")
library(readxl)
total <- read_excel("~/Library/Mobile Documents/com~apple~CloudDocs/Dissertation/data/total.xlsx")
subset_total <- total[, c("ccodealp","year", "dem_bi", "country", "IdealPointAll", "vdem_gender", 
                          "democratic_performance_name", "major_power", "regime_status_name", "nonwest", "gaiscore")]         ### Subset the specific columns
subset_total<-na.omit(subset_total) 

ungd_merge<-merge(UNGD, subset_total, by= c("ccodealp","year"), all = FALSE)                           ### merge variables from "total" to "UNGD"


UNGD_corpus <- corpus(ungd_merge, text_field = "text")  # creating quantida corpus 
summary(UNGD_corpus)


ungd_tokens <- tokens(UNGD_corpus,                                     ### tokenize 
                       what = "word",    
                       remove_punct = TRUE,                            ### remove punctuation  
                       remove_symbols = TRUE,                          ### remove symbols 
                       remove_numbers = TRUE,                          ### remove numbers 
                       remove_url = TRUE,                              ### remove urls 
                       remove_separators = TRUE)    


ungd_dfm<- dfm(ungd_tokens,                                           ### lowercase the terms and create DFM
                       tolower = TRUE)

ungd_dfm<<-  dfm_remove(ungd_dfm,                                     ### remove stopwords and unimportant words and symbols 
                        pattern = c(stopwords("english"), "--", "must", "like", "will", "also", "a1", "abba", "abdel", "'" , "-")) 

ungd_dfm<- dfm_wordstem(ungd_dfm,                                     ### stemming  
                        language = "en")    

ungd_dfm<- dfm_trim(ungd_dfm,
                      min_termfreq = 5)                              ### remove infrequent words
                      #max_docfreq = "")                             ### remove frequent words 


ungd2015 <- dfm_subset(ungd_dfm, year %in% 2001:2008) 
ungd_out <- quanteda::convert(ungd2015, to = "stm") ### convert DFM into a format compatible with the stm package (used for structural topic modeling).
ungd_docs <- ungd_out$documents                     ### extract the "documents" component from the ungd_out object.
ungd_vocab <- ungd_out$vocab                        ### extract the "vocab" (vocabulary) component from the ungd_out object    
ungd_meta <- ungd_out$meta                          ###  extract the "meta" (metadata) component from the ungd_outobject. Metadata typically contains document-level variables
```

## Running STM

```{r}
stm_model<- stm(documents = ungd_out$documents, # documents
               vocab = ungd_out$vocab,  # terms
               K = 5, # number of topics
               prevalence =~ IdealPointAll+vdem_gender+regime_status_name+gaiscore+s(year), # big difference vs LDA: a regression equation that models prevalence by actor, alliance, and year covariates. s means look in differnet years
               data = ungd_meta, 
               init.type = "LDA", # could also choose "LDA" here, but Spectral is faster. you could specify "LDA" as well. 
               seed = 12334567) # set your seed for replicability, results can change by seed

summary(stm_model)
```

## Topic quality

```{r}
topicQuality(model=stm_model, documents=ungd_docs)
```

## Topic Proportions

```{r}
plot.STM(stm_model,
         type="summary",
         labeltype = "frex",
         n = 5)
```

## Regression Analysis

```{r}
topics_model<- estimateEffect(1:5 ~ regime_status_name + gaiscore+ s(year), 
                                stm_model, 
                                meta = ungd_meta)
summary(topics_model)
```

## Changes in Topics over Time

```{r}
plot.estimateEffect(topics_model,
                    covariate = "year",
                    method = "continuous",
                    topics = 1)

plot.estimateEffect(topics_model,
                    covariate = "year",
                    method = "continuous",
                    topics = 2)

plot.estimateEffect(topics_model,
                    covariate = "year",
                    method = "continuous",
                    topics = 3)

plot.estimateEffect(topics_model,
                    covariate = "year",
                    method = "continuous",
                    topics = 4)

plot.estimateEffect(topics_model,
                    covariate = "year",
                    method = "continuous",
                    topics = 5)
```

## Choosing Topic Number

```{r}
topics_number<- searchK(documents = ungd_out$documents,  
                            vocab = ungd_out$vocab, 
                            K = c(4:8), 
                            prevalence =~ regime_status_name + gaiscore+ s(year),  
                            data = ungd_meta,
                            seed = 12334567)
```

## Plotting "optimal" topic numbers

```{r}
plot(topics_number)
```
